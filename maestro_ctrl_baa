#!/usr/bin/env python3
import os, sys
import errno
import yaml
import argparse
import json
import time
import copy
from collections import Mapping, Sequence
from maestro_util import *

class Cluster(object):
    def emit_value(self, path, value):
        try:
            res = client.put(path, str(value))
        except Exception as e:
            print("Error {0} setting {1} : {2}".format(str(e), path, str(value)))

    def walk_tilde(self, obj, subs=None, debug=False):
        """Walk the obj given and apply ~{} string replacement in place.
           In scalar values, ~{var} is replaced by its value if
           var is the name of scalar value at the same or higher scope.
           This provides us the ability to say:
           cluster: foo
           config:
             - plugin: bar
             - inst: ~{cluster}/${HOSTNAME}/~{plugin}
           which is not a feature of yaml or json.
           Needed since yaml anchors do not substitute within strings.
        """
        if not subs:
            subs = TildeSubs()
        if isinstance(obj, (int, float, bool, str)):
            print(obj)
            print(dir(obj))
            return
        if isinstance(obj, Sequence):
            i = 0
            for v in obj:
                self.walk_tilde(v, subs=subs, debug=debug)
                if isinstance(v, str):
                    changed = True
                    newval = v
                    while changed:
                        (changed, newval) = subs.expand_val(newval, debug)
                    if newval != v:
                        obj[i] = newval
                i += 1
        if isinstance(obj, Mapping):
            subs.push(obj)
            for (key, value) in obj.items():
                self.walk_tilde(value, subs=subs, debug=debug)
            changed = True
            while changed:
                changed = False
                for (key, value) in obj.items():
                    if isinstance(value, str):
                        changed = changed or subs.expand_key(key, debug)
            subs.pop()

    def walk(self, obj, path=''):
        if obj is None:
            if path.split("/")[-1] in CORE_ATTRS:
                print(f'{path.split("/")[-1]} not present in ldms yaml configuration file.\nContinuing..')
        elif isinstance(obj, Mapping):
            for key in obj:
                self.walk(obj[key], '{0}/{1}'.format(path, str(key)))
        elif isinstance(obj, Sequence):
            if isinstance(obj, (str, bytearray)):
                self.emit_value(path, obj)
            else:
                item = 0
                for v in obj:
                    # we want keys to be returned in numerical order which requires z-fill
                    self.walk(v, path + '/{0:06}'.format(item))
                    item += 1
        elif obj:
            self.emit_value(path, obj)

    def build_daemons(self, config):
        """Generate a daemon spec list from YAML config

        Builds a dictionary of endpoint definitions. The 'config' is a
        list of daemon specifications. Each daemon specification contains
        'names', 'host', and 'endpoint' attributes. All attributes are
        expanded per the slurm hostlist rules. The length of the
        expanded name-list, must equal the length of the expanded
        host-list.

        Example:

        daemons:
          - names : "agg-[1-10]"
            hosts : "node[1-10]"
            endpoints :
              - names : "node-[1-10]-[10002]
                ports : "[10002]"
                maestro_comm : True
                xprt : sock
                auth :
                  name : munge

        results in the following host-spec dictionary:

        {
        "agg-[[1-3]-10002]" : [
          "node-1-10002" : { "host" : "node-1", "port" : 10002 },
          "node-2-10002" : { "host" : "node-2", "port" : 10002 },
          "node-3-10002" : { "host" : "node-3", "port" : 10002 },
          ...
        ]
        }

        """
        ep_dict = {}
        node_config = config['daemons']
        for spec in node_config:
            check_required([ 'names', 'endpoints', 'hosts' ],
                           spec, "daemons specification")
            hosts = expand_names(spec['hosts'])
            ep_dict[spec['names']] = {}
            for ep in spec['endpoints']:
                check_required([ 'names', 'ports' ],
                               ep, 'endpoint specification')
                xprt = check_opt('xprt', ep)
                auth_name = check_opt('auth', ep)
                auth_conf = check_opt('conf', ep)
                names = expand_names(ep['names'])
                ports = expand_names(ep['ports'])
                maestro_comm = parse_yaml_bool(check_opt('maestro_comm', ep))
                for host in hosts:
                    for port in ports:
                        name = names.pop(0)
                        h = {
                            'name' : name,
                            'addr' : host,
                            'port' : port,
                            'xprt' : xprt,
                            'maestro_comm' : maestro_comm,
                            'auth' : { 'name' : auth_name, 'conf' : auth_conf }
                        }
                        ep_dict[spec['names']][name] = h
        return ep_dict

    def build_groups(self, config):
        groups = {}
        if 'daemons' not in config:
            return groups
        group_conf = config['daemons']
        for group_spec in group_conf:
            check_required([ 'names', 'hosts', 'endpoints'],
                           group_spec, "daemon specification")
            endpoints = []
            for ep in group_spec['endpoints']:
                endpoints += expand_names(ep['names'])
            group_name = group_spec['names']
            hosts = expand_names(group_spec['hosts'])
            group = {
                'name'       : group_name,
                'hosts'      : hosts,
                'endpoints'  : endpoints,
            }
            groups[group_name] = group
        return groups

    def build_aggregators(self, config):
        aggregators = {}
        if 'aggregators' not in config:
            return aggregators
        agg_conf = config['aggregators']
        for agg_spec in agg_conf:
            check_required([ 'daemons' ],
                           agg_spec, "aggregator specification")
            names = expand_names(agg_spec['daemons'])
            group = agg_spec['daemons']
            endpoints = []
            daemons_ = None
            for daemons in config['daemons']:
                if group == daemons['names']:
                    daemons_ = daemons
            if daemons_ is None:
                raise ValueError(f"No daemons matched matched daemon key {group}")
            for eps in daemons_['endpoints']:
                if eps['maestro_comm'] is True:
                    endpoints += expand_names(eps['names'])
                else:
                    endpoints += expand_names(eps['names'])
            if len(names) != len(endpoints):
                print("NAMES: {}\nEPS: {}\n", names, endpoints)
                raise ValueError('"aggregators:" The "host" and "name" specifications must '
                                'expand to the same number of names')
            if group not in aggregators:
                aggregators[group] = []
            for name in names:
                endpoint = endpoints.pop(0)
                agg = {
                        'name'      : name,
                        'endpoint'  : endpoint,
                        'state'     : 'stopped' # 'running', 'error'
                }
                for k in ["file", "prolog", "epilog"]:
                    if k in agg_spec:
                        agg[k] = agg_spec[k]
                aggregators[group].append(agg)
        return aggregators

    def build_producers(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of producers in that group.
        """
        producers = {}
        for agg in config['aggregators']:
            if 'peers' not in agg:
                continue
            for prod in agg['peers']:
                check_required([ 'endpoints', 'updaters',
                                 'reconnect', 'type', ],
                               prod, '"producer" entry')
                # Use endpoints for producer names and remove names attribute?
                daemons = prod['daemons']
                names = expand_names(prod['endpoints'])
                endpoints = expand_names(prod['endpoints'])
                group = agg['daemons']
                if group not in producers:
                    producers[group] = {}

                subscribe = check_opt('subscribe', prod)
                if len(names) != len(endpoints):
                    raise ValueError('"producer": The "host" and "name" specifications must '
                                    'expand to the same number of strings')
                upd_spec = prod['updaters']
                # Expand and generate all the producers
                typ = prod['type']
                reconnect = cvt_intrvl_str_to_us(prod['reconnect'])
                for name in names:
                    endpoint = endpoints.pop(0)
                    prod = {
                        'daemons'   : daemons,
                        'name'      : name,
                        'endpoint'  : endpoint,
                        'type'      : typ,
                        'group'     : group,
                        'reconnect' : reconnect,
                        'updaters'  : upd_spec
                    }
                    if subscribe:
                        prod['subscribe'] = subscribe
                    producers[group][endpoint] = prod
        return producers

    def build_updaters(self, config):
        """
        Return a dictionary based on the aggregator. Each dictionary
        entry is a list of updaters in that group.
        """
        updaters = {}
        updtr_cnt = 0
        for agg in config['aggregators']:
            if 'peers' not in agg:
                continue
            for prod in agg['peers']:
                for updtr_spec in prod['updaters']:
                    check_required([ 'interval', 'sets', ],
                                   updtr_spec, '"updater" entry')
                    group = agg['daemons']
                    if group not in updaters:
                        updaters[group] = {}
                    grp_updaters = updaters[group]
                    updtr_name = f'updtr_{updtr_cnt}'
                    if updtr_name in grp_updaters:
                        raise ValueError(f"Duplicate updater name '{updtr_name}''. "\
                                         f"An updater name must be unique within the group")
                    updtr = {
                        'name'      : updtr_name,
                        'interval'  : cvt_intrvl_str_to_us(updtr_spec['interval']),
                        'group'     : agg['daemons'],
                        'sets'      : updtr_spec['sets'],
                        'producers' : [{ 'regex' : '.*' }]
                    }
                    if 'offset' in updtr_spec:
                        updtr['offset'] = cvt_intrvl_str_to_us(updtr_spec['offset'])
                    if 'auto' in updtr_spec and 'push' in updtr_spec:
                        raise ValueError(f"The updater specification: {json.dumps(updtr_spec)} "
                                          "contains both 'push' and 'auto' which are "
                                          "mutually exclusive")
                    if 'auto' in updtr_spec:
                        updtr['auto'] = updtr_spec['auto']
                    if 'push' in updtr_spec:
                        updtr['push'] = updtr_spec['push']
                    grp_updaters[updtr_name] = updtr
                    updtr_cnt += 1
        return updaters

    def build_stores(self, config):
        """
        Return a list. Each list entry is a list of stores in that group.
        Not enough information is available at this scope for enforcement
        of name uniqueness, thus a list and not a map.
        """
        if 'stores' not in config:
            return None
        stores = {}
        for store_spec in config['stores']:
            check_required([ 'name', 'plugin', 'container', 'schema' ],
                           store_spec, '"store" entry')
            group = store_spec['daemons']
            if group not in stores:
                stores[group] = []
            grp_stores = stores[group]
            check_required([ 'name', 'config'],
                           store_spec['plugin'],
                           '"store plugin" entry')
            grp_stores.append(store_spec)
        return stores

    def build_samplers(self, config):
        """
        Generate samplers from YAML config.
        Return a dictionary keyed by the samplers group name. Each dictionary
        entry is a single ldms daemon's sampler configuration.
        """
        if 'samplers' not in config:
            return None
        smplrs = {}
        for smplr_spec in config['samplers']:
            check_required([ 'daemons', 'plugins' ],
                           smplr_spec, '"sampler" entry')
            for plugin in smplr_spec['plugins']:
                check_required(['name'], plugin, 'plugin entry')
            smplrs[smplr_spec['daemons']] = smplr_spec
        return smplrs

    def build_plugins(self, config):
        """
        Generate plugins to load from a YAML config.
        Return a dictionary keyed by the plugin's group name. Each dictionary entry
        is a single plugin's configuration.
        """
        if 'plugins' not in config:
            return None
        plugins = {}
        for plugn_spec in config['plugins']:
            check_required([ 'name', 'config' ],
                           plugn_spec, '"plugin" entry')
            group = plugn_spec['daemons']
            if group not in plugins:
                plugins[group] = {}
            grp_plugins = plugins[group]
            plugin_name = plugn_spec['name']
            if plugin_name in grp_plugins:
                raise ValueError(f'Duplicate plugin name "{plugin_name}". '
                                  'Plugin must be unique within a group.')
            grp_plugins[plugin_name] = plugn_spec
        return plugins

    def __init__(self, client, name, cluster_config):
        """
        """
        self.client = client
        self.name = name
        self.cluster_config = cluster_config
        # keep a config copy until we are sure we don't need unsubst round-trip
        self.cluster_config_raw = copy.deepcopy(cluster_config)
        # expand ~{} substitutions
        self.walk_tilde(self.cluster_config)
        print(yaml.dump(conf_spec, indent=4, default_flow_style=False))
        self.daemons = self.build_daemons(self.cluster_config)
        self.aggregators = self.build_aggregators(self.cluster_config)
        self.groups = self.build_groups(self.cluster_config)
        self.producers = self.build_producers(self.cluster_config)
        self.updaters = self.build_updaters(self.cluster_config)
        self.stores = self.build_stores(self.cluster_config)
        self.samplers = self.build_samplers(self.cluster_config)
        self.plugins = self.build_plugins(self.cluster_config)

    def commit(self):
        pass

    def save_config(self):
        try:
            self.client.delete_prefix('/' + self.name)
            self.walk(self.daemons, '/' + self.name + '/daemons')
            self.walk(self.aggregators, '/' + self.name + '/aggregators')
            self.walk(self.groups, '/' + self.name + '/groups')
            self.walk(self.producers, '/' + self.name + '/producers')
            self.walk(self.updaters, '/' + self.name + '/updaters')
            self.walk(self.stores, '/' + self.name + '/stores')
            self.walk(self.samplers, '/' + self.name + '/samplers')
            self.walk(self.plugins, '/' + self.name + '/plugins')
            self.client.put('/'+self.name+'/last_updated', str(time.time()))
        except Exception as e:
            a, b, c = sys.exc_info()
            print(str(e)+' '+str(c.tb_lineno))
            return 1

    def write_opt_attr(self, fd, attr, val, endline=True):
        # Include leading space
        if attr is not None:
            fd.write(f' {attr}={val}')
        if endline:
            fd.write(f'\n')

    def write_producers(self, fd, group_name):
        #Writes producer configuration to file fd
        if group_name in self.producers:
            ''' Balance samplers across aggregators '''
            prdcrs = []
            loop = round(len(self.producers[group_name]) / len(self.aggregators[group_name]))
            last_sampler = None
            for prdcr in self.producers[group_name]:
                prdcrs.append(prdcr)
            if not last_sampler:
                idx = 0
            else:
                idx = prdcrs.index(last_sampler) + 1
            prod_group = list(self.producers[group_name].keys())[idx:]
            smplr_group = self.producers[group_name][prod_group[0]]['daemons']
            i = 0
            auth = check_opt('auth', self.daemons[smplr_group][prod_group[0]])
            auth_opt = check_opt('conf', self.daemons[smplr_group][prod_group[0]])
            if auth:
                fd.write(f'auth_add name={auth}')
                self.write_opt_attr(fd, 'conf', auth_opt)
            for producer in prod_group:
                if i >= loop:
                    break
                regex = False
                producer = self.producers[group_name][producer]
                pname = producer['name']
                port = self.daemons[producer['daemons']][producer['endpoint']]['port']
                xprt = self.daemons[producer['daemons']][producer['endpoint']]['xprt']
                hostname = self.daemons[producer['daemons']][producer['endpoint']]['addr']
                ptype = producer['type']
                interval = producer['reconnect']
                fd.write(f'prdcr_add name={pname} '+
                         f'host={hostname} '+
                         f'port={port} '+
                         f'xprt={xprt} type={ptype} interval={interval}\n')
                last_sampler = pname
                i += 1
                if 'regex' in producer:
                    regex = True
                    fd.write(f'prdcr_start_regex regex={producer["regex"]}\n')
            fd.write(f'prdcr_start_regex regex=.*\n')

    def write_samplers(self, fd, smplr_group):
        for sampler in self.samplers[smplr_group]['plugins']:
            if 'config' in sampler:
                cfg_str = parse_to_cfg_str(sampler['config'])
            else:
                cfg_str = ""
            sname = sampler['name']
            interval = cvt_intrvl_str_to_us(sampler['interval'])
            fd.write(f'#ws\nload name={sname}\n')
            fd.write(f'config name={sname} {cfg_str}\n')
            fd.write(f'start name={sname} interval={interval}')
            offset = check_opt('offset', sampler)
            self.write_opt_attr(fd, 'offset', offset)

    def write_sampler_plugins(self, group_name):
        """dead code apparently"""
        print("write_sampler_plugins: group_name {} self {}".format(group_name, self))
        if self.plugins != None:
            if group_name in self.plugins:
                if 'file' in self.samplers[groupname]:
                    fname = self.samplers[groupname]['file']
                else:
                    fname = group_name
                fd = open(f'{path}/{group_name}-samplers.conf.wsp', 'w+')
                for plugin in self.plugins[group_name]:
                    cfg_str = parse_to_cfg_str(self.plugins[group_name][plugin]['config'])
                    fd.write(f'#wsp\nload name={plugin}\n')
                    fd.write(f'config name={plugin} {cfg_str}\n\n')
                fd.close()
        else:
            print("write_sampler_plugins: nothing to do\n")

    def write_agg_plugins(self, fd, group_name):
        # Write independent plugin configuration for group <group_name>
        if self.plugins != None:
            if group_name in self.plugins:
                for plugin in self.plugins[group_name]:
                    cfg_str = parse_to_cfg_str(self.plugins[group_name][plugin]['config'])
                    if 'stream' in self.plugins[group_name][plugin]['config']:
                        if 'regex' in self.plugins[group_name][plugin]['config']:
                            regex = self.plugins[group_name][plugin]['config']
                            fd.write(f'prdcr_subscribe stream={self.plugins[group_name][plugin]["config"]["stream"]} '\
                                     f'regex={regex}\n')
                        else:
                            fd.write(f'prdcr_subscribe stream={self.plugins[group_name][plugin]["config"]["stream"]} '\
                                     f'regex=.*\n')
                    fd.write(f'load name={plugin}\n')
                    fd.write(f'config name={plugin} {cfg_str}\n\n')

    def write_updaters(self, fd, group_name):
        if group_name in self.updaters:
            updtr_group = self.updaters[group_name]
            for updtr in updtr_group:
                interval = cvt_intrvl_str_to_us(updtr_group[updtr]['interval'])
                updtr_str = f'updtr_add name={updtr_group[updtr]["name"]} '
                if 'mode' in updtr_group[updtr]:
                    mode = updtr_group[updtr]['mode']
                else:
                    mode = 'static'
                # Check mode
                if mode == 'push':
                    updtr_str += 'push=onchange\n'
                elif mode == 'auto_interval':
                    updtr_str += 'auto_interval=True\n'
                fd.write(f'updtr_add name={updtr_group[updtr]["name"]} '+
                         f'interval={interval}')
                offset = check_opt('offset', updtr_group[updtr])
                self.write_opt_attr(fd, 'offset', offset)
                for prod in updtr_group[updtr]['producers']:
                    fd.write(f'updtr_prdcr_add name={updtr_group[updtr]["name"]} '+
                             f'regex={prod["regex"]}\n')
                fd.write(f'updtr_start name={updtr_group[updtr]["name"]}\n')

    def write_stores(self, fd, group_name):
        if group_name in self.stores:
            store_group = self.stores[group_name]
            loaded_plugins = []
            for store in store_group:
                if store["plugin"]["name"] not in loaded_plugins:
                    fd.write(f'load name={store["plugin"]["name"]}\n')
                    fd.write(f'config name={store["plugin"]["name"]} '+
                             f'{parse_to_cfg_str(store["plugin"]["config"])}\n')
                    loaded_plugins.append(store["plugin"]["name"])
                # enable store policy default with ~{schema}
                store_name = self.map_tilde(store, "name")
                strgp_add = f'#\nstrgp_add name={store_name} plugin={store["plugin"]["name"]} '
                strgp_add += f'container={store["container"]} '
                strgp_add += f'schema={store["schema"]}'
                fd.write(strgp_add)
                # handle optional flush, metrics, and producer_add in strgp
                flush = check_opt('flush', store)
                if flush:
                    self.write_opt_attr(fd, 'flush', flush, endline=False)
                fd.write("\n")
                if 'metrics' in store:
                    print("store: metrics = {}\n".format(store["metrics"]))
                    for met in store['metrics']:
                        strgp_metric_add = f'strgp_metric_add name={store_name} '
                        strgp_metric_add += f'{met}\n'
                        fd.write(strgp_metric_add)
                if 'producers' in store:
                    print("store: producers = {}\n".format(store["producers"]))
                    for ex in store['producers']:
                        strgp_prdcr_add = f'strgp_prdcr_add name={store_name} '
                        strgp_prdcr_add += f'{parse_to_cfg_str(ex)}\n'
                        fd.write(strgp_prdcr_add)
                fd.write(f'strgp_start name={store_name}\n')


    def map_tilde(self, d, key):
        """expand ~{} valued names from other items in the same map d."""
        if not key in d:
            return None
        import re
        if not hasattr(self, 're_tilde'):
            self.re_tilde = re.compile(r'~\{[^~{}]+\}')
        val = d[key]
        newval = ""
        last = 0
        changed = False
        for m in self.re_tilde.finditer(val):
            newval += val[last:m.start()]
            last = m.end()
            var = m.string[m.start():m.end()][2:-1]
            if var in d:
                s = d[var]
            else:
                s=""
            changed = True
            newval += str(s)
        newval += val[last:]
        return newval

    def config_v4(self, path):
        """
        Read the group configuration from ETCD and generate a version 4 LDMSD configuration
        This configuration assumes that the environemnt variables COMPONENT_ID, HOSTNAME
        all exist on the machines relevant to the ldmsd cluster.
        """
        for group_name in self.groups:
            # Load sampler statements
            group = self.groups[group_name]

            # Sampler config
            if self.samplers != None:
                try:
                    self.write_sampler_plugins(group_name) ; # original call is just broken
                    for smplr_group in self.samplers:
                        # TO DO: Refactor sampler config architecture to more easily reference appropriate groups
                        if group_name != smplr_group:
                            continue
                        # provide output file name specification if optionally specified
                        if 'file' in self.samplers[smplr_group]:
                            fd = open(f'{path}/{self.samplers[smplr_group]["file"]}.conf', 'w+')
                        else:
                            fd = open(f'{path}/{group_name}-samplers.conf', 'w+')
                        fd.write(f'# from samplers: {{- daemons: {group_name}\n')
                        # include prolog and epilog so all things are easily done in maestro
                        if 'prolog' in self.samplers[smplr_group]:
                            fd.write(f'# prolog\n{self.samplers[smplr_group]["prolog"]}\n')
                        self.write_samplers(fd, smplr_group)
                        if 'epilog' in self.samplers[smplr_group]:
                            fd.write(f'# epilog\n{self.samplers[smplr_group]["epilog"]}\n')
                    if fd:
                        fd.close()
                except Exception as e:
                    a, b, d = sys.exc_info()
                    print(f'Error generating sampler configuration: {str(e)} {str(d.tb_lineno)}')
                    raise ValueError
            else:
                print(f'"samplers" not found in configuration file. Skipping...')

            # Agg config
            try:
                ''' "Balance" agg configuration if all samplers are included in each aggregator '''
                if group_name not in self.aggregators:
                    continue
                last_sampler = None
                for agg in self.aggregators[group_name]:
                    # provide output file name specification if optionally specified
                    if 'file' in agg:
                        fd = open(f'{path}/{agg["file"]}.conf', 'w+')
                    else:
                        fd = open(f'{path}/{group_name}-{agg["name"]}.conf', 'w+')
                    fd.write(f'# from aggregators: {{- daemons: {group_name} {agg["name"]}\n')
                    # include prolog and epilog so all things are easily done in maestro
                    if 'prolog' in agg:
                        fd.write(f'# prolog\n{agg["prolog"]}\n')
                    self.write_producers(fd, group_name)
                    self.write_agg_plugins(fd, group_name)
                    self.write_updaters(fd, group_name)
                    self.write_stores(fd, group_name)
                    if 'epilog' in agg:
                        fd.write(f'# epilog\n{agg["epilog"]}\n')
            except Exception as e:
                ea, eb, ec = sys.exc_info()
                print('Agg config Error: '+str(e)+' Line:'+str(ec.tb_lineno))
                raise ValueError

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Configuration")
    parser.add_argument("--cluster", metavar="ETCD_CONF", required=True,
                        help="The name of the etcd server cluster configuration file")
    parser.add_argument("--ldms_config", metavar="FILE", required=True,
                        help="The ldmsd services YAML configuration file. "
                        "This will not start the maestro load balancer")
    parser.add_argument("--prefix", metavar="STRING", required=True,
                        help="The prefix for the etcd database keys created from $FILE",
                        default="unknown")
    parser.add_argument("--generate-config-path", metavar="DIR_NAME", required=False,
                        help="Directory in which to dump ldmsd conf scripts. Old "
                        "ones are overwritten. (default: no conf script dump)",
                        default=False)
    parser.add_argument("--noetcd", action='store_true', default=False,
                        help="Create .noetcd file $FILE.noetcd to feed"
                        " maestro (default: use etcd)");
    parser.add_argument("--debug", action='store_true', default=False,
                        help="Enable debugging outputs");
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4"
                        "(v5 not yet implemented)",
                        default=4)
    args = parser.parse_args()

    # Load the cluster configuration file. This configures the daemons
    # that support the key/value configuration database
    etcd_fp = open(args.cluster)
    etcd_spec = yaml.safe_load(etcd_fp)

    pfx = etcd_spec['cluster']
    etcd_hosts = ()
    for h in etcd_spec['members']:
        etcd_hosts += (( h['host'], h['port'] ),)

    # All keys in the DB are prefixed with the cluster name, 'pfx'. So we can
    # have multiple monitoring hosted by the same consensus cluster.
    config_fp = open(args.ldms_config)
    conf_spec = yaml.safe_load(config_fp)
    if not conf_spec:
        print("empty {}. doing nothing.".format(args.ldms_config))
        sys.exit(1)
    if args.debug:
        # dump expanded yaml
        import pprint
        f=open(args.ldms_config+".debug" , 'w+')
        pp = pprint.PrettyPrinter(indent=4, stream=f, compact=True)
        pp.pprint(conf_spec)
        print("wrote {}\n".format(args.ldms_config+".debug" ))

    # Use the 1st host for now
    if args.noetcd:
        # from noetcd import NoetcdClient
        print("create noetcd3 client\n")
        client = NoetcdClient()
    else:
        import etcd3
        print("create etcd3 client\n")
        client = etcd3.client(host=etcd_hosts[0][0], port=etcd_hosts[0][1],
        grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                       ('grpc.max_receive_message_length',16*1024*1024)])

    cluster = Cluster(client, args.prefix, conf_spec)
    if args.generate_config_path:
        cluster.config_v4(args.generate_config_path)
        print("LDMSD v4 config files generated")
        # sys.exit(0)

    # blow away the existing configuration
    rc = cluster.save_config()
    if rc:
        print("Error saving ldms cluster configuration to etcd cluster")
        sys.exit(0)
    print("LDMS aggregator configuration saved to etcd cluster.")
    if args.noetcd:
        # dump to reloadable noetcd file since there is no etcd daemon db
        client.save_yaml(args.ldms_config + ".noetcd")
        print("wrote " + args.ldms_config + ".noetcd")

    sys.exit(0)
